---
title: "**Report_10657769_10651875**"
author: "By: Aderibigbe Olugbenga & Paul Hazell"
date: "18/04/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(caret)
library(e1071)
library(class)
library(randomForest)
library(tree)
```
# 3.1 Machine Learning Task

__*Part (a) Selecting parameters*__

```{r data, include=FALSE}
orchid <- read.table(url("https://gist.githubusercontent.com/CptnCrumble/e01af3b83ffc463f4bb5776d0213f14b/raw/5382eee21b6e5b796541fd8053c5f733fd6eb9c7/orchids.txt"))
attach(orchid)
```

Graph of bivariate scatter plots to distinguish between the three locations of Orchids     
  
   
     
```{r, include=TRUE, echo=FALSE}
ggplot(orchid, aes( x = X1, y = X2)) +
  geom_point(aes (color = factor(loc)), shape = 16, size = 2)+
  theme_classic()+
  labs(title ="Orchids graph", x = "Petal length (mm)", y = "Leaf width (mm)", color = "Orchids Location\n")
```
   
   
Boxplots of the data to choose two characteristcs that should be used as predictors for orchids' locations. 

```{r, include=TRUE, echo=FALSE}
boxplot(X1 ~ loc, 
        xlab = "Location", 
        ylab = "Petal length",
        col = c("#F8766D", "#7CAE00", "#00BFC4"))

boxplot(X2 ~ loc, 
        xlab = "Location", 
        ylab = "Leaf width",
        col = c("#F8766D", "#7CAE00", "#00BFC4"))

boxplot(X3 ~ loc, 
        xlab = "Location", 
        ylab = "Petal width",
        col = c("#F8766D", "#7CAE00", "#00BFC4"))
```

From the graph it can be seen that there is a considerable difference between the mean values of the Petal length (X1) and Leaf width (X2).

```{r, include=TRUE, echo=FALSE}
aggregate(X1 ~ loc, FUN = mean)
aggregate(X2 ~ loc, FUN = mean)
```


Whereas the difference between the mean Petal width (X3) with different locations is not much.   
```{r, include=TRUE, echo=FALSE}
aggregate(X3 ~ loc, FUN = mean)
```
   
   
So the Petal length and Leaf width data will be used as predictors for the orchids' locations.

---

__*Part (b) Training data*__

Creating a training set 210 randomly chosen
data points and a test set of 60 data points.
```{r, include=TRUE, echo=TRUE}
set.seed(1)
data.subset <- sample(270, 210)
model.train <- orchid[data.subset,]
model.test <- orchid[-data.subset,]
```

---

__*Part (c) KNN Method*__


```{r, include=TRUE, echo=TRUE}
# Create knn model
set.seed(1)
model.knn <- train(loc~.-X3, 
                   data = model.train, 
                   method = "knn", 
                   trControl = trainControl(method  = "LOOCV"),
                   preProcess = c("center", "scale"), # Normalize the data
                   tuneLength = 10) # Number of possible K values to evaluate
```
   
Graph showing the accuracy of K   
   
```{r, include=TRUE, echo=FALSE}
plot(model.knn, main="Leave-One-Out Cross-Validation against size of K",xlab="K",ylab="Accuracy")
```

The best optimal k value is `r model.knn$bestTune`  


```{r, include=TRUE, echo=TRUE}
# Predicting the test model
predict.knn <- model.knn %>% predict(model.test)
```

```{r, include=TRUE, echo=FALSE}
pl = seq(min(model.test$X1), max(model.test$X1), by=0.1)
pw = seq(min(model.test$X2), max(model.test$X2), by=0.1)

# generates the boundaries for the graph
lgrid <- expand.grid(X1=pl, X2=pw, X3=19.73)

knnPredGrid <- predict(model.knn, newdata=lgrid)

knnPredGrid <- model.knn %>% predict(lgrid)

knnPredGrid = as.numeric(knnPredGrid)

predict.knn <- as.numeric(predict.knn)

model.test$loc <- predict.knn

probs <- matrix(knnPredGrid, 
                length(pl), 
                length(pw))

colour_map <- function(x){
  if(x==1){return("#F8766D")}
  else if(x==2){return("#7CAE00")}
  else{return("#00BFC4")}
}

plot_colors<- apply(probs,1:2,colour_map)
  
contour(pl, pw, probs, labels="", 
        xlab="Petal length (mm)", ylab="leaf width (mm)", 
        main="KNN predictions of Orchid location", axes=T)

gd <- expand.grid(x=pl, y=pw)

points(gd, pch=3, col=plot_colors, cex= 0.1)

# add the test points to the graph

plot_points <- sapply(model.test$loc,colour_map)

points(model.test$X1, model.test$X2, col= plot_points, cex= 2, pch = 20) 
```

---

__*Part (d) Random Forest Bagging method*__

```{r, include=TRUE, echo=FALSE}
# Create Random Forest Bagging Model
set.seed(1)
bag.tree <- randomForest(loc ~ . -X3, data = orchid, subset = data.subset,
                         mtry = 2, importance = TRUE)
round(importance(bag.tree), 2)
```

```{r, include=TRUE, echo=FALSE}
# Predicting the test data
bag_predict <- predict(bag.tree, model.test, type = "class")
```

```{r, include=TRUE, echo=FALSE}
lgrid <- expand.grid(X1=pl, X2=pw, X3=19.73)

bagPredGrid <- predict(bag.tree, newdata=lgrid)

bagPredGrid <- bag.tree %>% predict(lgrid)

bagPredGrid = as.numeric(bagPredGrid)

predict.bag <- as.numeric(bag_predict)

model.test$loc <- predict.bag

probs <- matrix(bagPredGrid, length(pl), length(pw))

contour(pl, pw, probs, labels="", 
        xlab="Petal length (mm)", ylab="Leaf width (mm)", 
        main="Random Forest Bagging method", axes=T)

gd <- expand.grid(x=pl, y=pw)

plot_colors<- apply(probs,1:2,colour_map)
points(gd, pch=3, col=plot_colors, cex=0.1)

# add the test points to the graph
plot_points <- sapply(model.test$loc,colour_map)
points(model.test$X1, model.test$X2, col= plot_points, cex= 2, pch = 20)
```

---

__*Part (e) Support Vector Machines*__

Linear kernel

```{r, include=TRUE, echo=TRUE}
set.seed(1)
tune.out = tune(svm, loc ~ X1 + X2, data = orchid[data.subset,],
                kernel ="linear",
                ranges = list(cost = seq(from = 0.01,to = 2, length = 40) ))
```

Choosing the best cost parameter  
  
    
```{r, include=TRUE, echo=TRUE}
plot(tune.out$performances$cost, tune.out$performances$error, main = "NEEDS A TITLE",xlab = "NEEDS LABEL",ylab = "NEEDS LABEL")
```

The best cost parameter for the linear kernel is `r tune.out$best.model$cost`

```{r, include=TRUE, echo=FALSE}
bestmod = tune.out$best.model
plot(bestmod, data = model.test, X2~X1)
```

```{r, include=TRUE, echo=TRUE}
# Predicting test result linear
ypred_linear = predict(bestmod, model.test)
```


Polynomial Kernel


```{r, include=TRUE, echo=TRUE}
# Create Polynomial Kernel
set.seed(1)
tune.out_poly = tune(svm, loc ~ X1 + X2, data = orchid[data.subset,],
                     kernel ="polynomial",
                     ranges = list(cost = seq(from = 0.01,to = 3, length = 30)))

```

Choosing the best cost parameter   
   
   
```{r, include=TRUE, echo=TRUE}
plot(tune.out_poly$performances$cost, tune.out_poly$performances$error, main = "NEEDS A TITLE",xlab = "NEEDS LABEL",ylab = "NEEDS LABEL")
```

The best cost parameter for the Polynomial kernel is `r tune.out_poly$best.model$cost`  

```{r, include=TRUE, echo=FALSE}
bestmod_poly = tune.out_poly$best.model
plot(bestmod_poly, data = model.test, X2~X1)
```

```{r, include=TRUE, echo=FALSE}
# Predicting test result Polynomial
ypred = predict(bestmod_poly, model.test)
```

Which kernel is better?
Computing the accuracy from the confusion matrix. It can be seen that the accuracy from the Linear Kernel is higher than that of the Polynomial Kernel. It would be best to choose the kernel with the higher accuracy which is the Linear Kernel
---

__*Part (f) Comparison*__

**Test Accuracy for KNN Method**    

```{r, include=TRUE, echo=FALSE}
# KNN method
tab <- table(predict.knn,model.test$loc)
tab
1-(sum(tab) - sum(diag(tab))) / sum(tab)
```

**Test Accuracy for Random Forest Bagging Method**    
   
```{r, include=TRUE, echo=FALSE}
# Random forest bagging method
tab.bag <- table(bag_predict,model.test$loc)
tab.bag
1-(sum(tab.bag) - sum(diag(tab.bag))) / sum(tab.bag)
```

**Test Accuracy for Linear Kernel Support Vector**   
   
```{r, include=TRUE, echo=FALSE}
# Support vector machine Linear
tab.linear <- table(ypred_linear,model.test$loc)
tab
1-(sum(tab.linear) - sum(diag(tab.linear))) / sum(tab.linear)
```

**Test Accuracy for Polynomial Kernel Support Vector**   
   
```{r, include=TRUE, echo=FALSE}
# Support vector machine Polynomial kernel
tab.poly <- table(ypred,model.test$loc)
tab.poly
1-(sum(tab.poly) - sum(diag(tab.poly))) / sum(tab.poly)
```

From our results it can be seen that the model with the best accuracy is the Random Forest Bagging method.    
This method came out as the best because the Bagging method helps in increasing the accuracy of the Random Forest method and also helps reduce variance of the model to help in predicting accurate results.

---
